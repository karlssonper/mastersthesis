\subsection {Designing the core library}

On an abstract level, the workflows in the GPU environemnts are more or less the same. This motivates the use of a library where a developer can target a common abstract interface and not have to worry about specific GPU environment implementations. The library is named \emph{gpuip} which stands for \emph {GPU Image Processing}. The core superclass in \emph{gpuip} is called {\tt ImageProcessor} and it is through this class that all the GPU communication is going to happen. An implementation of an {\tt ImageProcessor} needs to support the following common steps:

\begin{enumerate}
\item Compile kernel text code and build into GPU machine code.
\item Allocate memory on the GPU.
\item Copy data from CPU to GPU and vice versa.
\item Run kernels on the GPU.
\end{enumerate}

These steps are all virtual functions in the {\tt ImageProcessor} class that needs to be implemented in the subclasses. The class {\tt Buffer} provides memory allocation information to {\tt ImageProcessor}. An algorithm can have an arbitrary amount of buffers. The input and outputs are stored in buffers. Each buffer has a unique name and information about data type and how many channels there are per pixel. The supported data formats are {\tt unsigned byte, half} and {\tt float}. {\tt half} is a 16-bit floating point scalar in comparison to the standard 32-bit {\tt float}. Each buffer can have between one to four channels per pixel. Two or three channels per pixels is supported but not recommended as it leads to uncoalesced reads and writes[ref]. 
\newline
\renewcommand{\lstlistingname}{Code}
\begin{lstlisting}[caption= Buffer struct, label=bufferapi]
struct Buffer{
    typedef shared_ptr<Buffer> Ptr;
    enum Type { UNSIGNED_BYTE, HALF, FLOAT };
    const string name;
    Type type;
    unsigned int channels;
};
\end{lstlisting}
\newpage
The {\tt Kernel} class provides the following:
\begin{enumerate}
\item GPU kernel code. Syntax and program structure depends on GPU environment.
\item Which buffers are going to be used as input? What are they called in the kernels?
\item Which buffers are going to have data written to them?
\item Parameters used in the kernel code.
\end{enumerate}

\renewcommand{\lstlistingname}{Code}
\begin{lstlisting}[caption= Kernel struct, label=kernelapi]
struct Kernel {
    struct BufferLink {
        Buffer::Ptr buffer;
        string name;
    };
    typedef shared_ptr<Kernel> Ptr;
    const string name;
    string code;
    vector<BufferLink> inBuffers;
    vector<BufferLink> outBuffers;
    vector<Parameter> params;
};
\end{lstlisting}

As can been seen in Code \ref{bufferapi} and Code \ref{kernelapi}, both structs have a {\tt Ptr} typedef. A {\tt Buffer::Ptr} is a shared pointer[ref] to a {\tt Buffer} object. To register a {\tt Buffer} or a {\tt Kernel} to an {\tt ImageProcessor} object, the factory method pattern[ref] is used. An example of this can be seen in Code \ref{factoryfunc}.
\newline
\renewcommand{\lstlistingname}{Code}
\begin{lstlisting}[caption= Factory functions to create Buffer and Kernel objects, label=factoryfunc]
class ImageProcessor {
    ... rest of ImageProccessor definitions ...

    Buffer::Ptr CreateBuffer(const string & name,
                             Buffer::Type type,
                             unsigned int channels);

    Kernel::Ptr CreateKernel(const string & name);
};
\end{lstlisting}
In this case, the factory method pattern guarantees that the {\tt ImageProcessor} is the \emph{owner} of the buffers and kernel objects since it is going to store a copy of the shared pointer internally. As long as the {\tt ImageProcessor} exists, the {\tt Buffer} and {\tt Kernel} objects are also guaranteed to exist. This makes it impossible to reach cases where the {\tt ImageProcessor} is using pointers that are pointing to deleted objects and is considered a safe approach. Anyone using the library can still create and delete {\tt Buffer} and {\tt Kernel} objects on their own, but they will never be able to register these objects themselves.

To make things simple, all GPU operations are synchronous which means that once a function that communicates with the GPU has been been called, it is not going to return until the GPU is done proceeding the task. There are asynchronous ways of calling the GPU in all environment but they require syncing stages. As discussed in [ref], it might be worth supporting asynchronous calls in the future since they could potentially give better performance.
\newline
\renewcommand{\lstlistingname}{Code}
\begin{lstlisting}[caption= {\tt ImageProcessor} API for GPU operations, label=ipapi]
class ImageProcessor {
    ... rest of ImageProccessor definitions ...

    virtual double Allocate(string * error);
    virtual double Build(string * error);
    virtual double Run(string * error);
    virtual double Copy(Buffer::Ptr buffer,
                        CopyOperationEnum operation,
                        void * data,
                        string * error);
};
\end{lstlisting}

As can be seen in Code \ref{ipapi}, all functions that perform GPU operations returns the execution time as a {\tt double} and takes a pointer to an error {\tt string}. If an error occurs inside one of these functions, a \emph{negative} value is returned and the error message can be found in the error string. 

One of the goals when designing \emph{gpuip} is to be an easy-to-use library. {\tt ImageProcessor} is the superclass and GLSL, CUDA and OpenCL would implemented as subclasses. The most common approach in object-oriented programming is to put subclasses in different header and source files. To use a certain implementation later on, one would have to include the header file of the implementation and create the subclass object itself (but store it as an pointer to the superclass for flexbility between different implementations. A typical user-case can be seen in Code \ref{includeex}.
\renewcommand{\lstlistingname}{Code}
\begin{lstlisting}[caption= Example of common use of object-oriented programming, label=includeex]
#include <gpuip-cuda.h>
#include <gpuip-glsl.h>
#include <gpuip-opencl.h>

void some_function(string environment)
{
  ImageProcessor ip;
  if(environment == 'cuda') {
    ip = new gpuip::CUDA();
  } else if(environment == 'glsl') {
    ip = new gpuip::OpenCL();
  } else if(environment == 'opencl') {
    ip = new gpuip::GLSL();
  }

  ...
}
\end{lstlisting}

Designing gpuip like the example in Code \ref{includeex} would make the public interface less clean, involve different header files and could potentially motivate the use of pointer casting which is not ideal[ref]. Instead, the public interface of gpuip consinsts of only one header file and the only exposed class is the {\tt ImageProcessor} class. Under the hood, there are still going to be subclasses, but the the developer using the library will not notice this. This enforces that all implementations always use the same function calls and work the same way. The use of factory functions are used once again, this time static as they are placed within the class itself.
\newline
\begin{lstlisting}[caption= {\tt ImageProcessor} static API, label=ipapi]
enum Environment { OPENCL, CUDA, GLSL };
class ImageProcessor {
    ... rest of ImageProccessor definitions ...

  static ImageProcessor::Ptr Create(Environment env);
  static bool CanCreate(Environemt env);
};
\end{lstlisting}

To make this work, we include all the implemented subclasses in the {\tt ImageProcessor} source file. An example of this can be seen in Code \ref{forw}
\newline
\begin{lstlisting}[caption= {\tt ImageProcessor::Create} spawns instances of subclasses to itself, label=ipapi]
// gpuip.cpp
#include <gpuip-cuda.h> 
static ImageProcessor::Ptr Create(Environment env)
{
  if (env == CUDA) {
    return ImageProcessor::Ptr(new CUDAImpl());
  }
  ...
}
\end{lstlisting}


