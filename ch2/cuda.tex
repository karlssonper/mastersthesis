\newpage
\subsection{CUDA}

The CUDA architecture was released for the first time in 2006 to make it easier to perform general purpose GPU-computing. Unlike previous GLSL methods that had to get around the shaders pipeline, CUDA allowed every aritmethic logic unit on the chip to be controlled by a CUDA-program. Another important feature was the possibility to read and write to arbitrary memory address on the GPU in comparison to previous methods that required textures as storage. The hardware was still in charge of memory caching but CUDA also exposed a software managed cache called shared memory. CUDA programs are written in the CUDA C language, a language very close to the C language with the exception of a small number of keywords added for special features in the CUDA architecture.  

\subsubsection{Blocks and threads}

In the CUDA architecture, threads are single execution units that run kernels on the GPU. They are similar to CPU threads but the typically there are a lot more of them on the GPU. The threads are divided into thread blocks. Threads within a thread block can communicate with each others. The number of blocks and threads per block is decided by the developer when the kernel is called. The grid of thread blocks can be one, two or three dimensional. The maximum number of blocks and threads per block is decided by the GPU and its hardware. A CUDA kernel is executed simultaneously by warps. A warp consists of threads within a block. Typically each warp has the size of 32 threads, where actions like memory read and writes are performed in half-warps. 
\newline

\begin{figure}[ht!]
\centering
\includegraphics[width=80mm]{img/cuda.pdf}
\caption{CUDA threads divided into blocks of threads.}
\label{cudablockthreads}
\end{figure}

\renewcommand{\lstlistingname}{Code}
\begin{lstlisting}[caption= Example of vector addition in CUDA, label=cuda1]
__global__ void VectorAddition(float * A, 
                               float * B, 
                               float * C)
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    int j = blockIdx.y * blockDim.y + threadIdx.y;
    int idx = i + j * gridDim.x * blockDim.x;
    C[idx] = A[idx] + B[idx];
}
\end{lstlisting}

When a thread is executing a kernel, there are built-in variables to access information about which block the thread belongs to and the local thread index in the actual block. This information is often used to decide indices in global arrays for read and write operations. Code \ref{cuda1} shows an example of a simple CUDA kernel performing a vector addition. The keyword \texttt{\_\_global\_\_} is used to tell the compiler that the function is a CUDA kernel. The variables \texttt{blockIdx}, \texttt{threadIdx}, \texttt{gridDim} and \texttt{blockDim} are automatically built-in in a CUDA kernel and can be accessed at any time. \texttt{blockIdx} and \texttt{threadIdx} are of the type \texttt{uint3}, where each value represent an index in the corresponding dimension. \texttt{blockIdx} is the index of the block in the total grid of launched blocks and \texttt{threadIdx} is the thread index within the block. Both \texttt{gridDim} and \texttt{blockDim} are of the type \texttt{dim3} and are constant in all threads. It is not possible to launch a kernel with different block sizes.

\subsubsection{Memory hierarchy}
\begin{figure}[ht!]
\centering
\includegraphics[]{img/cuda2.pdf}
\caption{Different CUDA memory spaces}
\label{cudamemoryhierarchy}
\end{figure}
There are different memory spaces in the CUDA architecture as can be seen in Figure \ref{cudamemoryhierarchy}. Each thread has a local and private memory space. All the threads in a block share a memory space called shared memory. Shared memory is on-chip and is divided into different banks. Reading from shared memory in warp of threads is just as fast as reading from a register as long as there are no bank conflicts. Each bank conflict results in a serialized read from the shared memory. A worst case scenario is when all threads in a warp only read from one bank in the shared memory. The shared memory read then becomes a lot slower then the regular global memory. Global, constant and texture memory can be accessed by all threads. Constant and texture memory are only used in certain cases while the global memory is the common choice for storing data. The texture memory space is cached in a way where a texture fetch only cost a GPU device memory read on a cache miss, otherwise it only costs a read from the texture cache. Texture cache is optimized for 2D spatial locality. If desired, a read from the the texture memory gets automatic linear interpolation. Constant memory is a read-only memory space. All the threads of a half-warp read from constant memory just as fast as from a register as long as all the threads read from the same address. The cost scales linearly with the number of different addressed read by the threads. For example, the worst case would be if an array would be stored in the constant memory space and each thread in an executing half-warp needs a unique value in this array. The global memory space is capable of reading 4, 8 and 16 bytes of memory into registers in one single instruction. The most efficient global memory reads are when all threads in a half-warp reads from continous memory in a coalesced read of 64, 128 or 256 bytes. Within in a kernel, it is not possible to read and write to the same array. CUDA supports atomic operations but they are usually slow and the more common choice is to allocate multiple buffers, one to read from and one to write to.



