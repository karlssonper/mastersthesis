\section{Discussion}
\subsection{Comparison between the GPU environments}

All GPU environments have both their pros and cons. After working on the implementation of each GPU environment, these are the my summarized conclusions:

\begin{itemize}
\item OpenCL
\begin{description}
  \item[$+$] Supports runtime compilation.
  \item[$+$] Works on any GPU and any CPU.
  \item[$-$] Poor 16-bit floating point support.
  \item[$-$] Does not support templating.
\end{description}
\item CUDA
\begin{description}
  \item[$+$] Easy to use with runtime API.
  \item[$+$] Debugging tools.
  \item[$+$] Supports templating and operator overloading.
  \item[$-$] Does not support runtime compilation.
  \item[$-$] Only works on NVIDIA GPUs.
  \item[$-$] Poor 16-bit floating point support.
\end{description}
\item GLSL
\begin{description}
  \item[$+$] Works on any GPU. OpenGL is include in a lot of systems by default.
  \item[$+$] Supports runtime compilation.
  \item[$+$] Abstract way of dealing with data.
  \item[$+$] Display functionality for free.
  \item[$-$] Setup is not clean. Need to fake render a quad.
  \item[$-$] Can only write to one pixel at a time.
  \item[$-$] No local shared memory between work items.
\end{description}
\end{itemize}

If I had to pick only one GPU environment to use in a new application, I would choose OpenCL because it is very flexible and runs both on CPU and the GPU. GLSL would be an okay choice as long as the application is about image processing. GLSL should work out of the box on most systems and that makes it very easy to deploy the application to other parties. However, for more general purpose computing, GLSL quickly becomes bulky and requires a lot of tricks. Writing OpenCL and CUDA kernels is about the same and the setup in CUDA is actually easier than OpenCL. The limitation of NVIDIA-only GPUs is too big of a factor to me and if NVIDIA would make CUDA run on all GPUs, I think CUDA would increase a lot in popularity. All the tests in this thesis were very basic and there might be a possibility that CUDA is the best environment to use once you want to optimize further and really get the most out of the GPU.

\subsection{OpenGL interoperability}

Currently, if an image has been created/modified with gpuip and needs to be displayed, it first has to be copied back to the CPU and then uploaded back to the GPU for viewing. Both OpenCL and CUDA support OpenGL interoperability by mapping a GPU buffer to an OpenGL buffer. This means that data produced by OpenCL and CUDA can be used as an OpenGL texture without unnecessary transferring between the CPU and the GPU. Although more internal work, the public API for {\tt Buffer} would not change much:
\newline
\renewcommand{\lstlistingname}{Code}
\begin{lstlisting}[caption= gpuip OpenGL interoperability, label=glinter]
struct Buffer{
  // ... rest of Buffer declarations 
  bool glInteroperability;
  GLint glTexture;
};
\end{lstlisting}

I think this option would make the library more lucrative to use in applications that are using OpenGL for viewing graphics. One particular case I could see it being useful is in deferred rendering for realtime 3D graphics. Once geometry has been rendered to different textures, it might be faster to apply operations in OpenCL or CUDA than it is in GLSL (there is no support for sharing memory between execution threads in GLSL).

\subsection{Template kernels}

Consider a case where you only want to write one kernel file but support multiple fileformats. For GLSL, this is already true and quite practical. However, it is not possible in OpenCL and CUDA. CUDA supports templated functions. CUDA only supports {\tt half} storage and not computation and it is therefore not possible to template a function and have it work the same with {\tt half}, {\tt unsigned byte} and {\tt float}. In OpenCL, half computation is supported if the graphics drivers come with the {\tt cl\_khr\_fp16} extension. OpenCL does not support templates but since we parse the kernel code ourselves in gpuip, we could implement our own templating rules and generate one OpenCL kernel for every data type that gpuip supports. 

\subsection{Computations as input}

It is only possible to write data per pixel in gpuip. If an algorithm would require a global property of a buffer, like the maximum or average value, it would not be possible. For example, if someone wants to write a tone mapping algorithm, they might compute a tone mapping value per pixel and then use the average value of all pixels as input to a second step in the tone mapping. A kernel can have user-defined parameters but not parameters that depend on the output of a kernel. It might be nice to add an option to make it possible to use the computation of a buffer as input. To make things still fairly simple, it could be restricted to only allow computations of one-dimensional buffers. Then a computation could support operators such as min, max, median, avg and sum. Internally, gpuip would perform the GPU algorithms. For example, to get the minimum value of a one-dimensional buffer, it would have to run a reduce algorithm. It might be worth exploring the common libraries for the GPU computing. Using libraries like Boost Compute\cite{boostcompute} and Thrust\cite{cudathrust} would save time and probably have better performance.

\subsection{Work item distribution}

Some algorithms might be optimized further by allowing a work item/thread to write to more than one pixel. For example, in separable blur algorithms, it might be worth splitting the algorithm in two steps and have each work item operate on a row/column alone. Memory lookups tend to be the expensive part of an algorithm and if a work item can work alone on a row, data could be stored in the local registers as the work item iterates over the pixels. A different work item distribution is not possible in GLSL where every kernel has to be executed on a per pixel level.
